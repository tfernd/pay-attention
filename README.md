# Pay Attention

This repository provides a PyTorch implementation of various attention computation algorithms for efficient computation.

The goal of this repository is to provide a clean, efficient, and user-friendly implementation of attention mechanisms for developers and researchers. It currently includes a collection of attention mechanism implementations, and serves as a centralized hub for exploring efficient attention computation.

## Available functions

This repository currently implements the following attention mechanisms:

- [x] Standard attention (`standard`)
- [x] Batch/sequence chunked attention (`batch_chunked`, `sequence_chunked`, `batch_and_sequence_chunked`)
- [x] Memory-efficient attention (`memory_efficient`)
- [ ] xformers (`xformers`)
- [ ] ToME (`tome`)

The implementation has been tested with Python 3.9, and PyTorch 1.13.0.

## Getting started

To use the attention mechanisms implemented in this repository, follow the instructions below:

1. Clone the repository to your local machine.
2. Install the required dependencies by running pip install -r requirements.txt.

TODO

## Contributing
We welcome and encourage contributions to this repository! If you find a bug or want to suggest an improvement, please open an issue on GitHub. If you want to submit a patch or propose a new feature, please fork the repository and submit a pull request.

## License
This repository is licensed under the MIT license. See the LICENSE file for details.
